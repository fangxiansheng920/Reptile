# Reptile
爬虫总结

---

#### 一、基础操作：网页解析与标签提取
1. **获取网页源码**  
   使用 `requests.get()` 发送请求，结合状态码校验和异常处理：  
   ```python
   import requests
   from bs4 import BeautifulSoup

   url = 'http://example.com'
   headers = {'User-Agent': 'Mozilla/5.0'}  # 伪装浏览器头
   response = requests.get(url, headers=headers)
   response.raise_for_status()  # 自动处理HTTP错误
   html_content = response.text
   ```

2. **解析HTML与提取标签内容**  
   - **方法1**：使用 `find()` 和 `find_all()`  
     ```python
     soup = BeautifulSoup(html_content, 'lxml')
     titles = soup.find_all('h1', class_='title')  # 提取所有class为title的h1标签
     ```
   - **方法2**：使用CSS选择器（推荐高效写法）  
     ```python
     links = soup.select('div.content > a[href]')  # 提取div.content下所有带href的a标签
     ```

3. **处理动态属性与嵌套数据**  
   若标签属性动态生成或嵌套复杂，可用正则表达式或层级定位：  
   ```python
   import re
   # 查找所有包含data-属性的标签
   dynamic_tags = soup.find_all(attrs={'data-': True})
   # 嵌套数据提取（如提取作者信息）
   author = soup.find(text=re.compile("作者：")).find_next('em').text
   ```

---

#### 二、反爬策略与应对方案
1. **基础伪装**
   - **User-Agent和请求头综合伪装**：在复制要访问的网页请求中的User-Agent
     ```python
     headers = {
     'User-Agent': 网页请求中的User-Agent,
     'Accept': 'text/html,application/xhtml+xml',
     'Accept-Encoding': 'gzip, deflate',
     'Referer': 'https://www.google.com/'
      }
     ```
   - **User-Agent池（动态）**：随机切换浏览器标识，避免单一请求特征。  
     ```python
     from fake_useragent import UserAgent
     ua = UserAgent()
     headers = {'User-Agent': ua.random}  # 使用fake-useragent库生成随机UA
     ```
   - **IP代理池**：通过免费代理服务（如ProxyPool）轮换IP：  
     ```python
     proxies = {'http': 'http://10.10.1.10:3128', 'https': 'http://10.10.1.10:1080'}
     response = requests.get(url, proxies=proxies)
     ```

3. **高级绕过技巧**  
   - **随机延迟**：模拟人类操作间隔，避免高频请求：  
     ```python
     import time, random
     time.sleep(random.uniform(0.5, 3.0))  # 每次请求间隔0.5~3秒
     ```

4. **应对动态渲染**  
   若页面内容由JavaScript加载，需使用Selenium或Pyppeteer模拟浏览器：  
   ```python
   from selenium import webdriver
   driver = webdriver.Chrome()
   driver.get(url)
   dynamic_content = driver.page_source  # 获取渲染后的完整HTML
   ```
---

#### 三、分页爬取与多页处理
1. **分页机制分析**  
   - **URL参数型**：如豆瓣电影Top250的分页参数`start`：  
     ```python
     for page in range(0, 100, 25):
         url = f'https://movie.douban.com/top250?start={page}'
         # 爬取每页数据
     ```
   - **“下一页”链接型**：解析页面中的翻页按钮链接：  
     ```python
     next_page = soup.select_one('a.next-page')['href']
     ```

2. **自动化分页爬取**  
   结合循环与终止条件判断：  
   ```python
   while True:
       # 解析当前页数据
       next_page = soup.find('a', text='下一页')
       if not next_page:
           break
       url = base_url + next_page['href']  # 拼接完整URL
   ```

---

#### 四、图片爬取与存储
1. **提取图片URL并下载**  
   ```python
   img_tags = soup.find_all('img')
   for img in img_tags:
       img_url = img['src']
       # 处理相对URL
       if not img_url.startswith('http'):
           img_url = urljoin(url, img_url)  # 使用urllib.parse.urljoin
       # 下载图片
       img_data = requests.get(img_url).content
       with open(f'images/{img_url.split("/")[-1]}', 'wb') as f:
           f.write(img_data)
   ```

---
